{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea206360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IST769 Midterm\n",
    "#-----------------\n",
    "\n",
    "# You will turn this file in along with screenshots as instructed on Blackboard\n",
    "\n",
    "# YOUR NAME:\n",
    "# YOUR EMAIL:\n",
    "# YOUR SUID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3254652-59ff-490d-9e2d-00d9d77e2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "user = \"sidearm\"\n",
    "passwd = \"SU2orange!\"\n",
    "s3_bucket = \"gamestreams\"\n",
    "s3_server = \"http://minio:9000\"\n",
    "s3_access_key = \"minio\"\n",
    "s3_secret_key = passwd\n",
    "minio_pwd = \"SU2orange!\"\n",
    "mongo_uri = f\"mongodb://{user}:{passwd}@mongo:27017/admin?authSource=admin\"\n",
    "server_name = \"jdbc:sqlserver://mssql\"\n",
    "database_name = \"sidearmdb\"\n",
    "mssql_user = \"sa\"\n",
    "mssql_pw = passwd\n",
    "mssql_url = server_name + \";\" + \"databaseName=\" + database_name + \";encrypt=true;trustServerCertificate=true;\"\n",
    "\n",
    "jars = [\n",
    "    \"org.apache.hadoop:hadoop-aws:3.1.2\",\n",
    "    \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\",\n",
    "    \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\",\n",
    "    \"com.microsoft.sqlserver:mssql-jdbc:12.2.0.jre11\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "        .config(\"spark.jars.packages\",\",\".join(jars) )\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", s3_server ) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d93a740-0b7c-410f-a8ed-2ad5734c34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+------+\n",
      "| id|  name|number|shots|goals|teamid|\n",
      "+---+------+------+-----+-----+------+\n",
      "|  1|   sam|     6|   56|   23|   101|\n",
      "|  2| sarah|     1|   85|   34|   101|\n",
      "|  3| steve|     2|   60|   20|   101|\n",
      "|  4| stone|    13|   33|   10|   101|\n",
      "|  5|  sean|    17|   26|    9|   101|\n",
      "|  6|   sly|     8|   78|   15|   101|\n",
      "|  7|   sol|     9|   52|   20|   101|\n",
      "|  8| shree|     4|   20|    4|   101|\n",
      "|  9|shelly|    15|   10|    2|   101|\n",
      "| 10| swede|    10|   90|   50|   101|\n",
      "| 11| jimmy|     1|  100|   50|   205|\n",
      "| 12| julie|     9|   10|    0|   205|\n",
      "| 13| james|     2|   45|   15|   205|\n",
      "| 14|  jane|    15|   82|   46|   205|\n",
      "| 15| jimmy|    16|   42|   30|   205|\n",
      "| 16| julie|     8|   67|   32|   205|\n",
      "| 17| james|    17|   40|   14|   205|\n",
      "| 18|  jane|     3|   91|   40|   205|\n",
      "| 19| jimmy|     5|   78|   22|   205|\n",
      "| 20| julie|    22|   83|   19|   205|\n",
      "+---+------+------+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HOW TO READ FROM MSSQL\n",
    "df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"teams\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .load()\n",
    "\n",
    "df2 = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"players\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .load()\n",
    "\n",
    "df2.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"teams\")\n",
    "df2.createOrReplaceTempView(\"players\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2486f5fd-62fe-41c7-b042-53098cb6da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO WRITE TO MSSQL\n",
    "df.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"players2\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60bb340d-d6db-419e-9eb2-55f53f079cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|  0 59:51 101 2 0|\n",
      "|  1 57:06 101 6 0|\n",
      "|  2 56:13 205 8 1|\n",
      "|  3 55:25 101 4 0|\n",
      "|  4 55:03 101 1 1|\n",
      "| 5 54:50 101 17 0|\n",
      "|  6 54:14 205 8 0|\n",
      "|  7 53:59 101 9 0|\n",
      "|  8 53:23 101 2 0|\n",
      "| 9 51:21 101 13 0|\n",
      "| 10 49:55 101 1 1|\n",
      "| 11 49:28 101 2 1|\n",
      "|12 48:52 101 10 1|\n",
      "| 13 47:52 101 4 1|\n",
      "| 14 47:44 101 9 0|\n",
      "| 15 46:38 101 2 0|\n",
      "| 16 45:49 101 1 1|\n",
      "| 17 45:31 101 4 0|\n",
      "| 18 43:29 205 1 1|\n",
      "| 19 41:54 205 1 1|\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Read the gamestream.txt from minio\n",
    "\n",
    "df3= f\"s3a://{s3_bucket}/gamestream.txt\"\n",
    "gamestrm = spark.read.text(df3)\n",
    "gamestrm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb788d2f-d58f-4d42-bc40-124facbb8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write the gamestream to mongodb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd637a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab3eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3565b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS TO EXAM QUESTIONS\n",
    "# PLACE YOUR ANSWERS BELOW TURN THIS .ipynb FILE IN ALONG WITH SCREENSHOTS AS INSTRUCTED ON BLACKBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59dff5f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (649560965.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_61/649560965.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    select t.name as Team_Name,t.wins asTeam_Wins,t.losses as Team_Losses,\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "\n",
    "select t.name as Team_Name,t.wins asTeam_Wins,t.losses as Team_Losses,\n",
    "p.name as Player_Name,p.shots as Player_Shots,p.goals as Player_Goals from teams t \n",
    "join players p on t.id = p.teamid\n",
    "\n",
    "#EXPLANATION\n",
    "## The above code gives team and player data\n",
    "## I used Join function to join both temans and players table\n",
    "## This is intended to display the required columns mentioned on the qn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc76e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "\n",
    "select columns[0] as Event_ID, columns[1] as Event_Timestamp, columns[2] as Team_ID, columns[3] as Player_Jersey_Num,\n",
    "columns[4] as `Goal/Miss` from gamestreams.`gamestream.txt`\n",
    "\n",
    "##EXPLANATION:\n",
    "##The question asks for columns to be labelled based on data dictionary,\n",
    "## I used columns[n] systax as text files can be read column by column\n",
    "## And it makes it easy to name them according to their column names from data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35cf344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------+-----------------+---------+\n",
      "|Event_Id|Event_Timestamp|Team_ID|Player_Jersey_Num|Goal/Miss|\n",
      "+--------+---------------+-------+-----------------+---------+\n",
      "|       0|          59:51|    101|                2|        0|\n",
      "|       1|          57:06|    101|                6|        0|\n",
      "|       2|          56:13|    205|                8|        1|\n",
      "|       3|          55:25|    101|                4|        0|\n",
      "|       4|          55:03|    101|                1|        1|\n",
      "|       5|          54:50|    101|               17|        0|\n",
      "|       6|          54:14|    205|                8|        0|\n",
      "|       7|          53:59|    101|                9|        0|\n",
      "|       8|          53:23|    101|                2|        0|\n",
      "|       9|          51:21|    101|               13|        0|\n",
      "|      10|          49:55|    101|                1|        1|\n",
      "|      11|          49:28|    101|                2|        1|\n",
      "|      12|          48:52|    101|               10|        1|\n",
      "|      13|          47:52|    101|                4|        1|\n",
      "|      14|          47:44|    101|                9|        0|\n",
      "|      15|          46:38|    101|                2|        0|\n",
      "|      16|          45:49|    101|                1|        1|\n",
      "|      17|          45:31|    101|                4|        0|\n",
      "|      18|          43:29|    205|                1|        1|\n",
      "|      19|          41:54|    205|                1|        1|\n",
      "|      20|          41:09|    101|                1|        1|\n",
      "|      21|          41:03|    205|                5|        1|\n",
      "|      22|          41:02|    101|               13|        0|\n",
      "|      23|          40:47|    101|               15|        0|\n",
      "|      24|          40:20|    101|                1|        1|\n",
      "|      25|          39:09|    101|                9|        0|\n",
      "|      26|          38:06|    101|                4|        0|\n",
      "|      27|          38:03|    101|               13|        1|\n",
      "|      28|          37:48|    205|               15|        1|\n",
      "|      29|          37:31|    205|               15|        1|\n",
      "|      30|          37:02|    101|               15|        0|\n",
      "|      31|          36:31|    205|                2|        0|\n",
      "|      32|          34:33|    101|                2|        0|\n",
      "|      33|          34:22|    101|               15|        1|\n",
      "|      34|          33:15|    101|                4|        0|\n",
      "|      35|          32:23|    205|                9|        0|\n",
      "|      36|          31:38|    101|                8|        0|\n",
      "|      37|          30:45|    101|                6|        0|\n",
      "|      38|          30:31|    101|               10|        0|\n",
      "|      39|          30:26|    101|               13|        0|\n",
      "|      40|          30:07|    101|                8|        0|\n",
      "|      41|          29:04|    205|               17|        0|\n",
      "|      42|          27:56|    205|               16|        0|\n",
      "|      43|          25:56|    205|               22|        0|\n",
      "|      44|          25:26|    205|                2|        0|\n",
      "|      45|          25:20|    205|                9|        0|\n",
      "|      46|          25:17|    101|               13|        0|\n",
      "|      47|          24:57|    101|                6|        1|\n",
      "|      48|          23:57|    101|                6|        1|\n",
      "|      49|          22:36|    205|                9|        0|\n",
      "|      50|          20:48|    205|               17|        0|\n",
      "|      51|          20:43|    205|                9|        0|\n",
      "|      52|          20:22|    101|                1|        1|\n",
      "|      53|          19:04|    205|                3|        0|\n",
      "|      54|          18:24|    101|                1|        0|\n",
      "|      55|          17:23|    205|                1|        1|\n",
      "|      56|          15:29|    101|                9|        0|\n",
      "|      57|          13:18|    101|                8|        0|\n",
      "|      58|          13:12|    101|               10|        0|\n",
      "|      59|          13:07|    205|                2|        1|\n",
      "|      60|          12:32|    205|               17|        1|\n",
      "|      61|          11:33|    101|                2|        0|\n",
      "|      62|          09:05|    101|                2|        1|\n",
      "|      63|          07:42|    101|               13|        0|\n",
      "|      64|          07:09|    101|               17|        0|\n",
      "|      65|          03:41|    205|                5|        0|\n",
      "|      66|          03:24|    101|                9|        0|\n",
      "|      67|          03:22|    101|                1|        0|\n",
      "|      68|          00:42|    101|               13|        0|\n",
      "|      69|          00:17|    101|                8|        0|\n",
      "|      70|          00:00|      0|                0|        0|\n",
      "+--------+---------------+-------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q3\n",
    "# Write pyspark code (in SQL or DataFrame API) to display the gamestream. \n",
    "# Label each of the columns in the gamestream with their appropriate columns names from the data dictionary.\n",
    "\n",
    "df3 = spark.read.option(\"header\",False).option(\"delimiter\",\" \")\\\n",
    ".option(\"inferSchema\",True).csv(f\"s3a://{s3_bucket}/gamestream.txt\")\\\n",
    ".toDF(\"Event_Id\",\"Event_Timestamp\",\"Team_ID\",\"Player_Jersey_Num\",\"Goal/Miss\")\\\n",
    ".createOrReplaceTempView(\"gs_view\")\n",
    "\n",
    "\n",
    "spark.sql(\"select * from gs_view\").show(n= 100)\n",
    "\n",
    "#EXPLANATION:\n",
    "# I used the reference from the Homework exercise on this.\n",
    "# I loaded the txt file from the minio bucket with specifying the delimiter and inferSchema so that it will retain its\n",
    "# variable types as it is. Then I loaded this to .csv file and converted to DataFrame using DF function and included all headers as \n",
    "#mentioned in the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea5d68ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+---------------------+----------+\n",
      "|Team_ID|Player_Jersey_Num|total_shots_Players|goals_for_each_player|Total_Goal|\n",
      "+-------+-----------------+-------------------+---------------------+----------+\n",
      "|    205|                3|                  1|                    0|        22|\n",
      "|    205|                1|                  3|                    3|        22|\n",
      "|    205|               15|                  2|                    2|        22|\n",
      "|    101|               13|                  7|                    1|        48|\n",
      "|    205|               22|                  1|                    0|        22|\n",
      "|    101|                9|                  5|                    0|        48|\n",
      "|    101|               17|                  2|                    0|        48|\n",
      "|    205|               17|                  3|                    1|        22|\n",
      "|    101|                8|                  4|                    0|        48|\n",
      "|    101|               10|                  3|                    1|        48|\n",
      "|    205|                2|                  3|                    1|        22|\n",
      "|    101|                1|                  8|                    6|        48|\n",
      "|    101|               15|                  3|                    1|        48|\n",
      "|    205|                9|                  4|                    0|        22|\n",
      "|    101|                4|                  5|                    1|        48|\n",
      "|    205|               16|                  1|                    0|        22|\n",
      "|    101|                2|                  7|                    2|        48|\n",
      "|      0|                0|                  1|                    0|         1|\n",
      "|    205|                8|                  2|                    1|        22|\n",
      "|    205|                5|                  2|                    1|        22|\n",
      "+-------+-----------------+-------------------+---------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4\n",
    "#Write pyspark code (in SQL or DataFrame API) to group the gamestream by team/player jersey number adding up the shots and goals. Specifically:\n",
    "\n",
    "#One row per team / jersey number in the gamestream.\n",
    "#Values dependent on team and player: total shots and goals for each player.\n",
    "#Value dependent on only team: total goals (this should repeat for every row with the same team id)\n",
    "query = '''\n",
    "with sc1 as(\n",
    "select Team_ID,count(`Goal/Miss`) as Total_Goal from gs_view\n",
    "group by Team_ID\n",
    ")\n",
    "select g.Team_ID,g.Player_Jersey_Num, count(g.`Goal/Miss`) as total_shots_Players ,sum(g.`Goal/Miss`) as goals_for_each_player, s.Total_Goal  from gs_view g join sc1 s on \n",
    "g.Team_ID = s.Team_ID\n",
    "group by g.Team_ID,g.Player_Jersey_Num, s.Total_Goal\n",
    "'''\n",
    "spark.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86db4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "## Use your output from 3. to include the most most current event id and timestamp for that point in time in the game. Same row level as 3. but now you include the latest event_id and timestamp.\n",
    "\n",
    "##For example (sample - not the actual data):\n",
    "query = '''\n",
    "with sc1 as(\n",
    "select Team_ID,count(`Goal/Miss`) as Total_Goal from gs_view\n",
    "group by Team_ID\n",
    "),\n",
    "sc2 as(\n",
    "select g.Team_ID,g.Player_Jersey_Num, count(g.`Goal/Miss`) as total_shots_Players,\n",
    "sum(g.`Goal/Miss`) as goals_for_each_player, s.Total_Goal  from gs_view g join sc1 s on \n",
    "g.Team_ID = s.Team_ID\n",
    "group by g.Team_ID,g.Player_Jersey_Num, s.Total_Goal \n",
    ")\n",
    "\n",
    "Select b.Latest_EventID,b.Latest_Timestamp,g.Team_ID,g.Player_Jersey_Num,g.total_shots_Players,\n",
    "g.goals_for_each_player, g.Total_Goal from sc2 g Join (Select MAX(Event_Id) as Latest_EventID, MAX(Event_Timestamp) as Latest_Timestamp from gs_view ) b on 1=1\n",
    "\n",
    "\n",
    "'''\n",
    "spark.sql(query).createOrReplaceTempView(\"box_score\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8cc24d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------+-----------------+-------------------+---------------------+----------+\n",
      "|Latest_EventID|Latest_Timestamp|Team_ID|Player_Jersey_Num|total_shots_Players|goals_for_each_player|Total_Goal|\n",
      "+--------------+----------------+-------+-----------------+-------------------+---------------------+----------+\n",
      "|            70|           59:51|    205|                3|                  1|                    0|        22|\n",
      "|            70|           59:51|    205|                1|                  3|                    3|        22|\n",
      "|            70|           59:51|    205|               15|                  2|                    2|        22|\n",
      "|            70|           59:51|    101|               13|                  7|                    1|        48|\n",
      "|            70|           59:51|    205|               22|                  1|                    0|        22|\n",
      "|            70|           59:51|    101|                9|                  5|                    0|        48|\n",
      "|            70|           59:51|    101|               17|                  2|                    0|        48|\n",
      "|            70|           59:51|    205|               17|                  3|                    1|        22|\n",
      "|            70|           59:51|    101|                8|                  4|                    0|        48|\n",
      "|            70|           59:51|    101|               10|                  3|                    1|        48|\n",
      "|            70|           59:51|    205|                2|                  3|                    1|        22|\n",
      "|            70|           59:51|    101|                1|                  8|                    6|        48|\n",
      "|            70|           59:51|    101|               15|                  3|                    1|        48|\n",
      "|            70|           59:51|    205|                9|                  4|                    0|        22|\n",
      "|            70|           59:51|    101|                4|                  5|                    1|        48|\n",
      "|            70|           59:51|    205|               16|                  1|                    0|        22|\n",
      "|            70|           59:51|    101|                2|                  7|                    2|        48|\n",
      "|            70|           59:51|      0|                0|                  1|                    0|         1|\n",
      "|            70|           59:51|    205|                8|                  2|                    1|        22|\n",
      "|            70|           59:51|    205|                5|                  2|                    1|        22|\n",
      "+--------------+----------------+-------+-----------------+-------------------+---------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q6\n",
    "# Write pyspark code (in SQL or DataFrame API) to join the output from question 4 with \n",
    "#the player and team reference data mssql so that you have the data necessary for the box score.\n",
    "\n",
    "query = '''\n",
    "\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca6d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7\n",
    "#Write pyspark code (in SQL or DataFrame API) to transform the output from question 5 into the box score document structure shown in part 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8\n",
    "\n",
    "#Write pyspark code (in SQL or DataFrame API) to write the box score completed in question 6 to the mongo.sidearm.boxscores collection.\n",
    "#The document should be keyed by event_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10472e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9\n",
    "#Combine parts 4-7 into a single pyspark script that will run the entire process of creating the box score document. \n",
    "#Make sure to run this a couple of times while the game stream is going on so there are at least 5 box score events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10\n",
    "#Write a drill SQL query to display the latest box score. The latest value should be derived from the data. not hard-coded eg. 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039708a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q11\n",
    "#When the game is complete, write pyspark code (in SQL or DataFrame API) update the wins and losses for the teams in the teams table. \n",
    "#Specifically, load the teams table and update it, then display the updated data frame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q12\n",
    "#Write pyspark code (in SQL or DataFrame API) to write the updated in question 11 to a new mssql.sidearmdb.teams2 table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b963d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q13\n",
    "#When the game is complete, write pyspark code (in SQL or DataFrame API) update the shots and goals for the players in the players table. \n",
    "#Specifically, load the players table and update it, then display the updated data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q14\n",
    "#Write pyspark code (in SQL or DataFrame API) to write the updated in question 11 to a new mssql.sidearmdb.players2 table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q15\n",
    "#Re-write drill SQL query from question 1 to use the updated players2 and teams2 tables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
