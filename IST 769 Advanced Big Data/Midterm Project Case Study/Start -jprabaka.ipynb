{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea206360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IST769 Midterm\n",
    "#-----------------\n",
    "\n",
    "# You will turn this file in along with screenshots as instructed on Blackboard\n",
    "\n",
    "# YOUR NAME: JAYA VARSHINI PRABAKAR\n",
    "# YOUR EMAIL: jprabaka@syr.edu\n",
    "# YOUR SUID: 463948991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3254652-59ff-490d-9e2d-00d9d77e2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "user = \"mongo\"\n",
    "passwd = \"SU2orange!\"\n",
    "s3_bucket = \"gamestreams\"\n",
    "s3_server = \"http://minio:9000\"\n",
    "s3_access_key = \"minio\"\n",
    "s3_secret_key = passwd\n",
    "minio_pwd = \"SU2orange!\"\n",
    "mongo_uri = f\"mongodb://{user}:{passwd}@mongo:27017/admin?authSource=admin\"\n",
    "server_name = \"jdbc:sqlserver://mssql\"\n",
    "database_name = \"sidearmdb\"\n",
    "mssql_user = \"sa\"\n",
    "mssql_pw = passwd\n",
    "mssql_url = server_name + \";\" + \"databaseName=\" + database_name + \";encrypt=true;trustServerCertificate=true;\"\n",
    "\n",
    "jars = [\n",
    "    \"org.apache.hadoop:hadoop-aws:3.1.2\",\n",
    "    \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\",\n",
    "    \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\",\n",
    "    \"com.microsoft.sqlserver:mssql-jdbc:12.2.0.jre11\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "        .config(\"spark.jars.packages\",\",\".join(jars) )\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", s3_server ) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d93a740-0b7c-410f-a8ed-2ad5734c34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+------+\n",
      "| id|  name|number|shots|goals|teamid|\n",
      "+---+------+------+-----+-----+------+\n",
      "|  1|   sam|     6|   56|   23|   101|\n",
      "|  2| sarah|     1|   85|   34|   101|\n",
      "|  3| steve|     2|   60|   20|   101|\n",
      "|  4| stone|    13|   33|   10|   101|\n",
      "|  5|  sean|    17|   26|    9|   101|\n",
      "|  6|   sly|     8|   78|   15|   101|\n",
      "|  7|   sol|     9|   52|   20|   101|\n",
      "|  8| shree|     4|   20|    4|   101|\n",
      "|  9|shelly|    15|   10|    2|   101|\n",
      "| 10| swede|    10|   90|   50|   101|\n",
      "| 11| jimmy|     1|  100|   50|   205|\n",
      "| 12| julie|     9|   10|    0|   205|\n",
      "| 13| james|     2|   45|   15|   205|\n",
      "| 14|  jane|    15|   82|   46|   205|\n",
      "| 15| jimmy|    16|   42|   30|   205|\n",
      "| 16| julie|     8|   67|   32|   205|\n",
      "| 17| james|    17|   40|   14|   205|\n",
      "| 18|  jane|     3|   91|   40|   205|\n",
      "| 19| jimmy|     5|   78|   22|   205|\n",
      "| 20| julie|    22|   83|   19|   205|\n",
      "+---+------+------+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HOW TO READ FROM MSSQL\n",
    "df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"teams\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .load()\n",
    "\n",
    "df2 = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"players\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .load()\n",
    "\n",
    "df2.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"teams\")\n",
    "df2.createOrReplaceTempView(\"players\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2486f5fd-62fe-41c7-b042-53098cb6da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO WRITE TO MSSQL\n",
    "df.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"players2\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60bb340d-d6db-419e-9eb2-55f53f079cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|  0 59:51 101 2 0|\n",
      "|  1 57:06 101 6 0|\n",
      "|  2 56:13 205 8 1|\n",
      "|  3 55:25 101 4 0|\n",
      "|  4 55:03 101 1 1|\n",
      "| 5 54:50 101 17 0|\n",
      "|  6 54:14 205 8 0|\n",
      "|  7 53:59 101 9 0|\n",
      "|  8 53:23 101 2 0|\n",
      "| 9 51:21 101 13 0|\n",
      "| 10 49:55 101 1 1|\n",
      "| 11 49:28 101 2 1|\n",
      "|12 48:52 101 10 1|\n",
      "| 13 47:52 101 4 1|\n",
      "| 14 47:44 101 9 0|\n",
      "| 15 46:38 101 2 0|\n",
      "| 16 45:49 101 1 1|\n",
      "| 17 45:31 101 4 0|\n",
      "| 18 43:29 205 1 1|\n",
      "| 19 41:54 205 1 1|\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Read the gamestream.txt from minio\n",
    "\n",
    "df3= f\"s3a://{s3_bucket}/gamestream.txt\"\n",
    "gamestrm = spark.read.text(df3)\n",
    "gamestrm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb788d2f-d58f-4d42-bc40-124facbb8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write the gamestream to mongodb\n",
    "gamestrm.write.format(\"mongo\").mode(\"overwrite\").option(\"database\",\"df\").option(\"collection\",\"gamestream\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd637a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab3eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3565b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS TO EXAM QUESTIONS\n",
    "# PLACE YOUR ANSWERS BELOW TURN THIS .ipynb FILE IN ALONG WITH SCREENSHOTS AS INSTRUCTED ON BLACKBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59dff5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "select t.name as Team_Name,t.wins asTeam_Wins,t.losses as Team_Losses,\n",
    "p.name as Player_Name,p.shots as Player_Shots,p.goals as Player_Goals from teams t \n",
    "join players p on t.id = p.teamid\n",
    "\n",
    "\n",
    "#EXPLANATION\n",
    "## The above code gives team and player data\n",
    "## I used Join function to join both temans and players table\n",
    "## This is intended to display the required columns mentioned on the qn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc76e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "\n",
    "select columns[0] as Event_ID, columns[1] as Event_Timestamp, columns[2] as Team_ID, columns[3] as Player_Jersey_Num,\n",
    "columns[4] as `Goal/Miss` from minio.`gamestream.txt`\n",
    "\n",
    "##EXPLANATION:\n",
    "##The question asks for columns to be labelled based on data dictionary,\n",
    "## I used columns[n] systax as text files can be read column by column\n",
    "## And it makes it easy to name them according to their column names from data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cf344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------+-----------------+---------+\n",
      "|Event_Id|Event_Timestamp|Team_ID|Player_Jersey_Num|Goal/Miss|\n",
      "+--------+---------------+-------+-----------------+---------+\n",
      "|       0|          59:51|    101|                2|        0|\n",
      "|       1|          57:06|    101|                6|        0|\n",
      "|       2|          56:13|    205|                8|        1|\n",
      "|       3|          55:25|    101|                4|        0|\n",
      "|       4|          55:03|    101|                1|        1|\n",
      "|       5|          54:50|    101|               17|        0|\n",
      "|       6|          54:14|    205|                8|        0|\n",
      "|       7|          53:59|    101|                9|        0|\n",
      "|       8|          53:23|    101|                2|        0|\n",
      "|       9|          51:21|    101|               13|        0|\n",
      "+--------+---------------+-------+-----------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q3\n",
    "# Write pyspark code (in SQL or DataFrame API) to display the gamestream. \n",
    "# Label each of the columns in the gamestream with their appropriate columns names from the data dictionary.\n",
    "\n",
    "df3 = spark.read.option(\"header\",False).option(\"delimiter\",\" \")\\\n",
    ".option(\"inferSchema\",True).csv(f\"s3a://{s3_bucket}/gamestream.txt\")\\\n",
    ".toDF(\"Event_Id\",\"Event_Timestamp\",\"Team_ID\",\"Player_Jersey_Num\",\"Goal/Miss\")\\\n",
    ".createOrReplaceTempView(\"gs_view\")\n",
    "\n",
    "\n",
    "spark.sql(\"select * from gs_view\").show(n= 10)\n",
    "\n",
    "#EXPLANATION:\n",
    "# I used the reference from the Homework exercise on this.\n",
    "# I loaded the txt file from the minio bucket with specifying the delimiter and inferSchema so that it will retain its\n",
    "# variable types as it is. Then I loaded this to .csv file and converted to DataFrame using DF function and included all headers as \n",
    "#mentioned in the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5d68ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+---------------------+----------+\n",
      "|Team_ID|Player_Jersey_Num|total_shots_Players|goals_for_each_player|Total_Goal|\n",
      "+-------+-----------------+-------------------+---------------------+----------+\n",
      "|      0|                0|                  1|                    0|         0|\n",
      "|    101|               17|                  2|                    0|        14|\n",
      "|    205|                9|                  4|                    0|         9|\n",
      "|    101|                4|                  5|                    1|        14|\n",
      "|    205|                8|                  2|                    1|         9|\n",
      "|    205|                5|                  2|                    1|         9|\n",
      "|    205|               22|                  1|                    0|         9|\n",
      "|    101|                8|                  4|                    0|        14|\n",
      "|    205|               17|                  3|                    1|         9|\n",
      "|    205|                2|                  3|                    1|         9|\n",
      "|    101|                1|                  8|                    6|        14|\n",
      "|    205|               16|                  1|                    0|         9|\n",
      "|    205|               15|                  2|                    2|         9|\n",
      "|    101|                9|                  5|                    0|        14|\n",
      "|    205|                1|                  3|                    3|         9|\n",
      "|    101|                2|                  7|                    2|        14|\n",
      "|    101|               15|                  3|                    1|        14|\n",
      "|    101|               10|                  3|                    1|        14|\n",
      "|    205|                3|                  1|                    0|         9|\n",
      "|    101|                6|                  4|                    2|        14|\n",
      "|    101|               13|                  7|                    1|        14|\n",
      "+-------+-----------------+-------------------+---------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4\n",
    "#Write pyspark code (in SQL or DataFrame API) to group the gamestream by team/player jersey number adding up the shots and goals. Specifically:\n",
    "\n",
    "#One row per team / jersey number in the gamestream.\n",
    "#Values dependent on team and player: total shots and goals for each player.\n",
    "#Value dependent on only team: total goals (this should repeat for every row with the same team id)\n",
    "query = '''\n",
    "with sc1 as(\n",
    "select Team_ID,sum(`Goal/Miss`) as Total_Goal from gs_view\n",
    "group by Team_ID\n",
    ")\n",
    "select g.Team_ID,g.Player_Jersey_Num, count(g.`Goal/Miss`) as total_shots_Players,\n",
    "sum(g.`Goal/Miss`) as goals_for_each_player,\n",
    "s.Total_Goal  from gs_view g join sc1 s on \n",
    "g.Team_ID = s.Team_ID\n",
    "group by g.Team_ID,g.Player_Jersey_Num, s.Total_Goal\n",
    "'''\n",
    "spark.sql(query).createOrReplaceTempView(\"player_dt\")\n",
    "spark.sql(query).show(n=30)\n",
    "#player_dt.show()\n",
    "\n",
    "# I started with a CTE to get Total_Goal since it should repeat for every team_id\n",
    "#So I grouped all team_id to get their Goals\n",
    "#Then I proceeded with player level and team level groups to get \n",
    "#shots by each player and Goals made by each player\n",
    "# to achieve this I counted Goal/Miss for each player to get my shots by each player \n",
    "# and I summed up Goal/Miss to gets goals scored by each player\n",
    "#Then I joined my CTE with Team_goal so that it will be repeated for every team on row level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86db4f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------+-----------------+-------------------+---------------------+----------+\n",
      "|Latest_EventID|Latest_Timestamp|Team_ID|Player_Jersey_Num|total_shots_Players|goals_for_each_player|Total_Goal|\n",
      "+--------------+----------------+-------+-----------------+-------------------+---------------------+----------+\n",
      "|            70|           59:51|      0|                0|                  1|                    0|         0|\n",
      "|            70|           59:51|    101|               17|                  2|                    0|        14|\n",
      "|            70|           59:51|    205|                9|                  4|                    0|         9|\n",
      "|            70|           59:51|    101|                4|                  5|                    1|        14|\n",
      "|            70|           59:51|    205|                8|                  2|                    1|         9|\n",
      "|            70|           59:51|    205|                5|                  2|                    1|         9|\n",
      "|            70|           59:51|    205|               22|                  1|                    0|         9|\n",
      "|            70|           59:51|    101|                8|                  4|                    0|        14|\n",
      "|            70|           59:51|    205|               17|                  3|                    1|         9|\n",
      "|            70|           59:51|    205|                2|                  3|                    1|         9|\n",
      "|            70|           59:51|    101|                1|                  8|                    6|        14|\n",
      "|            70|           59:51|    205|               16|                  1|                    0|         9|\n",
      "|            70|           59:51|    205|               15|                  2|                    2|         9|\n",
      "|            70|           59:51|    101|                9|                  5|                    0|        14|\n",
      "|            70|           59:51|    205|                1|                  3|                    3|         9|\n",
      "|            70|           59:51|    101|                2|                  7|                    2|        14|\n",
      "|            70|           59:51|    101|               15|                  3|                    1|        14|\n",
      "|            70|           59:51|    101|               10|                  3|                    1|        14|\n",
      "|            70|           59:51|    205|                3|                  1|                    0|         9|\n",
      "|            70|           59:51|    101|                6|                  4|                    2|        14|\n",
      "+--------------+----------------+-------+-----------------+-------------------+---------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q5\n",
    "## Use your output from 3. to include the most most current event id and timestamp for that point in time in the game. Same row level as 3. but now you include the latest event_id and timestamp.\n",
    "\n",
    "##For example (sample - not the actual data):\n",
    "query = '''\n",
    "\n",
    "Select b.Latest_EventID,b.Latest_Timestamp,g.Team_ID,g.Player_Jersey_Num,g.total_shots_Players,\n",
    "g.goals_for_each_player, g.Total_Goal from player_dt g \n",
    "Join (Select MAX(Event_Id) as Latest_EventID,\n",
    "MAX(Event_Timestamp) as Latest_Timestamp from gs_view ) b on 1=1\n",
    "'''\n",
    "spark.sql(query).show()\n",
    "spark.sql(query).createOrReplaceTempView(\"box_score\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8cc24d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------+----------+----+------+-----+---+------+-----+-------------------+---------------------+----+\n",
      "|Latest_EventID|Latest_Timestamp|Team_ID|conference|wins|losses|score| id|  name|shots|total_shots_Players|goals_for_each_player| pct|\n",
      "+--------------+----------------+-------+----------+----+------+-----+---+------+-----+-------------------+---------------------+----+\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  1|   sam|   56|                  8|                    6|0.75|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  1|   sam|   56|                  3|                    3| 1.0|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14| 13| james|   45|                  7|                    1|0.14|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  6|   sly|   78|                  4|                    2| 0.5|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9| 16| julie|   67|                  1|                    0| 0.0|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  3| steve|   60|                  1|                    0| 0.0|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  5|  sean|   26|                  2|                    1| 0.5|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14| 15| jimmy|   42|                  3|                    1|0.33|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9| 15| jimmy|   42|                  2|                    2| 1.0|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  9|shelly|   10|                  5|                    0| 0.0|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  9|shelly|   10|                  4|                    0| 0.0|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14| 17| james|   40|                  2|                    0| 0.0|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9| 17| james|   40|                  3|                    1|0.33|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  4| stone|   33|                  5|                    1| 0.2|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  8| shree|   20|                  4|                    0| 0.0|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  8| shree|   20|                  2|                    1| 0.5|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14| 10| swede|   90|                  3|                    1|0.33|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  2| sarah|   85|                  7|                    2|0.29|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  2| sarah|   85|                  3|                    1|0.33|\n",
      "+--------------+----------------+-------+----------+----+------+-----+---+------+-----+-------------------+---------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q6\n",
    "# Write pyspark code (in SQL or DataFrame API) to join the output from question 4 with \n",
    "#the player and team reference data mssql so that you have the data necessary for the box score.\n",
    "\n",
    "query = '''\n",
    "WITH box_team as(SELECT b.Latest_EventID, b.Latest_Timestamp,\n",
    "b.Team_ID , t.conference, t.wins, t.losses,\n",
    "b.Player_Jersey_Num,\n",
    "b.Total_Goal as score,\n",
    "b.total_shots_Players,\n",
    "b.goals_for_each_player\n",
    "from box_score b join teams t on\n",
    "t.id = b.Team_ID)\n",
    "\n",
    "SELECT\n",
    "    b.Latest_EventID, \n",
    "    b.Latest_Timestamp,\n",
    "    b.Team_ID , \n",
    "    b.conference,\n",
    "    b.wins, b.losses,\n",
    "    b.score,\n",
    "    p.id,\n",
    "    p.name,\n",
    "    p.shots,\n",
    "    b.total_shots_Players,\n",
    "    b.goals_for_each_player,\n",
    "    Round((b.goals_for_each_player/b.total_shots_Players),2) as pct\n",
    "    from box_team b join players p\n",
    "    on b.Player_Jersey_Num = p.id\n",
    "'''\n",
    "box_score_final=spark.sql(query)\n",
    "box_score_final.show()\n",
    "#spark.sql(query).createOrReplaceTempView(\"boxscore_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bca6d2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------+----------+----+------+-----+---+------+-----+-------------------+---------------------+----+---------+-----------+\n",
      "|Latest_EventID|Latest_Timestamp|Team_ID|conference|wins|losses|score| id|  name|shots|total_shots_Players|goals_for_each_player| pct|team_type|Team_Status|\n",
      "+--------------+----------------+-------+----------+----+------+-----+---+------+-----+-------------------+---------------------+----+---------+-----------+\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  1|   sam|   56|                  8|                    6|0.75|     home|    Winning|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14| 13| james|   45|                  7|                    1|0.14|     home|    Winning|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  6|   sly|   78|                  4|                    2| 0.5|     home|    Winning|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14| 15| jimmy|   42|                  3|                    1|0.33|     home|    Winning|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  9|shelly|   10|                  5|                    0| 0.0|     home|    Winning|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14| 17| james|   40|                  2|                    0| 0.0|     home|    Winning|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  4| stone|   33|                  5|                    1| 0.2|     home|    Winning|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  8| shree|   20|                  4|                    0| 0.0|     home|    Winning|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14| 10| swede|   90|                  3|                    1|0.33|     home|    Winning|\n",
      "|            70|           59:51|    101|       acc|  11|     2|   14|  2| sarah|   85|                  7|                    2|0.29|     home|    Winning|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  1|   sam|   56|                  3|                    3| 1.0|     away|     Losing|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9| 16| julie|   67|                  1|                    0| 0.0|     away|     Losing|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  3| steve|   60|                  1|                    0| 0.0|     away|     Losing|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  5|  sean|   26|                  2|                    1| 0.5|     away|     Losing|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9| 15| jimmy|   42|                  2|                    2| 1.0|     away|     Losing|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  9|shelly|   10|                  4|                    0| 0.0|     away|     Losing|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9| 17| james|   40|                  3|                    1|0.33|     away|     Losing|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  8| shree|   20|                  2|                    1| 0.5|     away|     Losing|\n",
      "|            70|           59:51|    205|     big10|   9|     4|    9|  2| sarah|   85|                  3|                    1|0.33|     away|     Losing|\n",
      "+--------------+----------------+-------+----------+----+------+-----+---+------+-----+-------------------+---------------------+----+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q7\n",
    "#Write pyspark code (in SQL or DataFrame API) to transform the output from question 5 into the box score document structure shown in part 3.1.\n",
    "# Add a new column to identify if the team is home or away\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, struct, sum as spark_sum, when, round, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Add a new column to identify if the team is home or away\n",
    "box_score_final = box_score_final.withColumn(\"team_type\", when(col(\"Team_ID\") == 101, \"home\").when(col(\"Team_ID\")==205,\"away\"))\n",
    "box_score_final.createOrReplaceTempView(\"box_score_final\")\n",
    "                                             \n",
    "# Show the resulting DataFrame\n",
    "#box_score_final.show()\n",
    "query = '''\n",
    "WITH st as(\n",
    "SELECT Team_ID,\n",
    "       CASE \n",
    "           WHEN Team_ID = 101 THEN\n",
    "               CASE \n",
    "                   WHEN Max(score) > (SELECT Max(score) FROM box_score_final WHERE Team_ID = 205) \n",
    "                   THEN 'Winning'\n",
    "                   WHEN Max(score) < (SELECT Max(score) FROM box_score_final WHERE Team_ID = 205) \n",
    "                   THEN 'Losing'\n",
    "                   ELSE 'Tied'\n",
    "               END\n",
    "           WHEN Team_ID = 205 THEN\n",
    "               CASE \n",
    "                   WHEN Max(score) > (SELECT Max(score) FROM box_score_final WHERE Team_ID = 101) \n",
    "                   THEN 'Winning'\n",
    "                   WHEN Max(score) < (SELECT Max(score) FROM box_score_final WHERE Team_ID = 101) \n",
    "                   THEN 'Losing'\n",
    "                   ELSE 'Tied'\n",
    "               END\n",
    "       END AS Team_Status\n",
    "FROM box_score_final\n",
    "group by Team_ID)\n",
    "\n",
    "SELECT b.*, s.Team_Status from box_score_final b JOIN st s on b.Team_ID = s.Team_ID \n",
    "'''\n",
    "\n",
    "box_score_final2=spark.sql(query)\n",
    "box_score_final2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "325bb87e-f7cf-46d5-93d3-71b3379833c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 374:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------+\n",
      "|_id|                away|                home|timestamp|\n",
      "+---+--------------------+--------------------+---------+\n",
      "| 70|[{205, big10, 9, ...|[{101, acc, 11, 2...|    59:51|\n",
      "+---+--------------------+--------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, struct \n",
    "#Games Latest eventId and Timestamp\n",
    "lastest_EventID = box_score_final2.select('Latest_EventID').distinct() \n",
    "latest_Timestamp = box_score_final2.select('Latest_Timestamp').distinct() \n",
    "#lastest_EventID.show()\n",
    "\n",
    "#Grouping everything so that we can achieve part 3.1 JSON documentation format\n",
    "box_doc = box_score_final2.groupBy(\"Team_ID\",\"conference\",\"wins\",\"losses\",\"score\",\"Team_Status\")\\\n",
    ".agg(collect_list(struct(\"id\",\"name\",\"shots\",\"total_shots_Players\",\"goals_for_each_player\")))\n",
    " \n",
    "# Section for Home and Away\n",
    "home = box_doc.filter(\"Team_ID == 101\")\n",
    "away = box_doc.filter(\"Team_ID== 205\")\n",
    "#home.show()\n",
    "#Document Structure\n",
    "mongo_strct = [\n",
    "{'_id': lastest_EventID.collect()[0][0],\n",
    "'timestamp': latest_Timestamp.collect()[0][0],\n",
    "'home': [home.first()],\n",
    "'away':[away.first()]}]\n",
    "#Creating a Dataframe\n",
    "mongo_df = spark.createDataFrame(mongo_strct)\n",
    "mongo_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff5f9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8\n",
    "\n",
    "#Write pyspark code (in SQL or DataFrame API) to write the box score completed in question 6 to the mongo.sidearm.boxscores collection.\n",
    "#The document should be keyed by event_id.\n",
    "mongo_df.write.format(\"mongo\").mode(\"append\").option(\"database\",\"sidearm\").option(\"collection\",\"boxscores\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10472e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------+\n",
      "|_id|                away|                home|timestamp|\n",
      "+---+--------------------+--------------------+---------+\n",
      "| 70|[{205, big10, 9, ...|[{101, acc, 11, 2...|    59:51|\n",
      "+---+--------------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q9\n",
    "#Combine parts 4-7 into a single pyspark script that will run the entire process of creating the box score document. \n",
    "#Make sure to run this a couple of times while the game stream is going on so there are at least 5 box score events.\n",
    "from pyspark.sql.functions import collect_list, struct\n",
    "\n",
    "query = '''\n",
    "with sc1 as(\n",
    "select Team_ID,sum(`Goal/Miss`) as Total_Goal from gs_view\n",
    "group by Team_ID\n",
    ")\n",
    "select g.Team_ID,g.Player_Jersey_Num, count(g.`Goal/Miss`) as total_shots_Players ,\n",
    "sum(g.`Goal/Miss`) as goals_for_each_player, \n",
    "s.Total_Goal  from gs_view g join sc1 s on \n",
    "g.Team_ID = s.Team_ID\n",
    "group by g.Team_ID,g.Player_Jersey_Num, s.Total_Goal\n",
    "'''\n",
    "player_dt = spark.sql(query)\n",
    "query2='''\n",
    "with sc1 as(\n",
    "select Team_ID,count(`Goal/Miss`) as Total_Goal from gs_view\n",
    "group by Team_ID\n",
    "),\n",
    "sc2 as(\n",
    "select g.Team_ID,g.Player_Jersey_Num, count(g.`Goal/Miss`) as total_shots_Players,\n",
    "sum(g.`Goal/Miss`) as goals_for_each_player, s.Total_Goal  from gs_view g join sc1 s on \n",
    "g.Team_ID = s.Team_ID\n",
    "group by g.Team_ID,g.Player_Jersey_Num, s.Total_Goal \n",
    ")\n",
    "\n",
    "\n",
    "Select b.Latest_EventID,b.Latest_Timestamp,g.Team_ID,g.Player_Jersey_Num,g.total_shots_Players,\n",
    "g.goals_for_each_player, g.Total_Goal from sc2 g Join (Select MAX(Event_Id) as Latest_EventID, \n",
    "MAX(Event_Timestamp) as Latest_Timestamp from gs_view ) b on 1=1\n",
    "'''\n",
    "box_score = spark.sql(query2)\n",
    "spark.sql(query2).createOrReplaceTempView(\"box_score\")\n",
    "\n",
    "query3='''\n",
    "WITH box_team as(SELECT b.Latest_EventID, b.Latest_Timestamp,\n",
    "b.Team_ID , t.conference, t.wins, t.losses,\n",
    "b.Player_Jersey_Num,\n",
    "b.Total_Goal as score,\n",
    "b.total_shots_Players,\n",
    "b.goals_for_each_player\n",
    "from box_score b join teams t on\n",
    "t.id = b.Team_ID)\n",
    "\n",
    "SELECT\n",
    "    b.Latest_EventID, \n",
    "    b.Latest_Timestamp,\n",
    "    b.Team_ID , \n",
    "    b.conference,\n",
    "    b.wins, b.losses,\n",
    "    b.score,\n",
    "    p.id,\n",
    "    p.name,\n",
    "    p.shots,\n",
    "    b.total_shots_Players,\n",
    "    b.goals_for_each_player,\n",
    "    Round((b.goals_for_each_player/b.total_shots_Players),2) as pct\n",
    "    from box_team b join players p\n",
    "    on b.Player_Jersey_Num = p.id\n",
    "    '''\n",
    "\n",
    "spark.sql(query3).createOrReplaceTempView(\"box_score_final\")\n",
    "query4='''\n",
    "WITH st as(\n",
    "SELECT Team_ID,\n",
    "       CASE \n",
    "           WHEN Team_ID = 101 THEN\n",
    "               CASE \n",
    "                   WHEN Max(score) > (SELECT Max(score) FROM box_score_final WHERE Team_ID = 205) \n",
    "                   THEN 'Winning'\n",
    "                   WHEN Max(score) < (SELECT Max(score) FROM box_score_final WHERE Team_ID = 205) \n",
    "                   THEN 'Losing'\n",
    "                   ELSE 'Tied'\n",
    "               END\n",
    "           WHEN Team_ID = 205 THEN\n",
    "               CASE \n",
    "                   WHEN Max(score) > (SELECT Max(score) FROM box_score_final WHERE Team_ID = 101) \n",
    "                   THEN 'Winning'\n",
    "                   WHEN Max(score) < (SELECT Max(score) FROM box_score_final WHERE Team_ID = 101) \n",
    "                   THEN 'Losing'\n",
    "                   ELSE 'Tied'\n",
    "               END\n",
    "       END AS Team_Status\n",
    "FROM box_score_final\n",
    "group by Team_ID)\n",
    "\n",
    "SELECT b.*, s.Team_Status from box_score_final b JOIN st s on b.Team_ID = s.Team_ID \n",
    "'''\n",
    "box_score_final2=spark.sql(query4)\n",
    " \n",
    "#Games Latest eventId and Timestamp\n",
    "lastest_EventID = box_score_final2.select('Latest_EventID').distinct() \n",
    "latest_Timestamp = box_score_final2.select('Latest_Timestamp').distinct() \n",
    "#lastest_EventID.show()\n",
    "\n",
    "#Grouping everything so that we can achieve part 3.1 JSON documentation format\n",
    "box_doc = box_score_final2.groupBy(\"Team_ID\",\"conference\",\"wins\",\"losses\",\"score\",\"Team_Status\")\\\n",
    ".agg(collect_list(struct(\"id\",\"name\",\"shots\",\"total_shots_Players\",\"goals_for_each_player\")))\n",
    " \n",
    "# Section for Home and Away\n",
    "home = box_doc.filter(\"Team_ID == 101\")\n",
    "away = box_doc.filter(\"Team_ID== 205\")\n",
    "#home.show()\n",
    "#Document Structure\n",
    "mongo_strct = [\n",
    "{'_id': lastest_EventID.collect()[0][0],\n",
    "'timestamp': latest_Timestamp.collect()[0][0],\n",
    "'home': [home.first()],\n",
    "'away':[away.first()]}]\n",
    "#Creating a Dataframe\n",
    "mongo_df = spark.createDataFrame(mongo_strct)\n",
    "mongo_df.show()\n",
    "\n",
    "#The document should be keyed by event_id.\n",
    "mongo_df.write.format(\"mongo\").mode(\"append\").option(\"database\",\"sidearm\").option(\"collection\",\"boxscores\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10\n",
    "#Write a drill SQL query to display the latest box score. The latest value should be derived from the data. not hard-coded eg. 56\n",
    "WITH max_id as(\n",
    "select MAX(`_id`) as max_id from mongo.`sidearm`.`boxscores` )\n",
    "select s.`_id`,s.`timestamp`,s.`home`,s.`away` from mongo.`sidearm`.`boxscores` s, max_id m\n",
    "where s.`_id` = m.max_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "039708a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+----+------+\n",
      "| id|         name|conference|Wins|losses|\n",
      "+---+-------------+----------+----+------+\n",
      "|101|     syracuse|       acc|  12|     2|\n",
      "|205|johns hopkins|     big10|   9|     5|\n",
      "+---+-------------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q11\n",
    "#When the game is complete, write pyspark code (in SQL or DataFrame API) update the wins and losses for the teams in the teams table. \n",
    "#Specifically, load the teams table and update it, then display the updated data frame.\n",
    "\n",
    "query ='''\n",
    "With scr1 as(\n",
    "SELECT Team_ID, first(score) as score from box_score_final\n",
    "group by Team_ID\n",
    "),\n",
    "\n",
    "team1 as(SELECT t.id,t.name, t.conference, t.wins+Case\n",
    "   WHEN\n",
    "    (SELECT score FROM scr1 WHERE team_id = 101) > \n",
    "    (SELECT score FROM scr1 WHERE team_id = 205)\n",
    "    THEN 1\n",
    "    ELSE 0 END as Wins\n",
    ",t.losses + Case\n",
    "WHEN\n",
    " (SELECT score FROM scr1 WHERE team_id = 101) <\n",
    "    (SELECT score FROM scr1 WHERE team_id = 205)\n",
    "    THEN 1\n",
    "    ELSE 0 END as losses\n",
    "\n",
    "from teams t join scr1 s on t.id = s.Team_ID\n",
    "where t.id = 101),\n",
    "\n",
    "team2 as(\n",
    "SELECT t.id,t.name, t.conference ,t.wins+Case\n",
    "   WHEN\n",
    "    (SELECT score FROM scr1 WHERE team_id = 101) <\n",
    "    (SELECT score FROM scr1 WHERE team_id = 205)\n",
    "    THEN 1\n",
    "    ELSE 0 END as Wins\n",
    ",t.losses + Case\n",
    "WHEN\n",
    " (SELECT score FROM scr1 WHERE team_id = 101) >\n",
    "    (SELECT score FROM scr1 WHERE team_id = 205)\n",
    "    THEN 1\n",
    "    ELSE 0 END as losses\n",
    "\n",
    "from teams t join scr1 s on t.id = s.Team_ID\n",
    "where t.id = 205\n",
    ")\n",
    "\n",
    "SELECT * from team1\n",
    "union\n",
    "SELECT * from team2\n",
    "\n",
    "\n",
    "'''\n",
    "updateTeams=spark.sql(query)\n",
    "updateTeams.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "473e48fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q12\n",
    "#Write pyspark code (in SQL or DataFrame API) to write the updated in question 11 to a new mssql.sidearmdb.teams2 table.\n",
    "\n",
    "updateTeams.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"teams2\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b963d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+------+\n",
      "| id|  name|number|shots|goals|teamid|\n",
      "+---+------+------+-----+-----+------+\n",
      "|  1|   sam|     6|   60|   27|   101|\n",
      "|  2| sarah|     1|   93|   42|   101|\n",
      "|  3| steve|     2|   67|   27|   101|\n",
      "|  4| stone|    13|   40|   17|   101|\n",
      "|  5|  sean|    17|   28|   11|   101|\n",
      "|  6|   sly|     8|   82|   19|   101|\n",
      "|  7|   sol|     9|   57|   25|   101|\n",
      "|  8| shree|     4|   25|    9|   101|\n",
      "|  9|shelly|    15|   13|    5|   101|\n",
      "| 10| swede|    10|   93|   53|   101|\n",
      "| 11| jimmy|     1|  103|   53|   205|\n",
      "| 12| julie|     9|   14|    4|   205|\n",
      "| 13| james|     2|   48|   18|   205|\n",
      "| 14|  jane|    15|   84|   48|   205|\n",
      "| 15| jimmy|    16|   43|   31|   205|\n",
      "| 16| julie|     8|   69|   34|   205|\n",
      "| 17| james|    17|   43|   17|   205|\n",
      "| 18|  jane|     3|   92|   41|   205|\n",
      "| 19| jimmy|     5|   80|   24|   205|\n",
      "| 20| julie|    22|   84|   20|   205|\n",
      "+---+------+------+-----+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q13\n",
    "#When the game is complete, write pyspark code (in SQL or DataFrame API) update the shots and goals for the players in the players table. \n",
    "#Specifically, load the players table and update it, then display the updated data frame.\n",
    "query = '''\n",
    "with player1 as(select g.Team_ID,g.Player_Jersey_Num, \n",
    "count(g.`Goal/Miss`) as total_shots_Players ,\n",
    "sum(g.`Goal/Miss`) as goals_for_each_player  from gs_view g\n",
    "group by g.Team_ID,g.Player_Jersey_Num)\n",
    "\n",
    "Select p.id, p.name, p.number, (p.shots+p1.total_shots_Players) as shots, \n",
    "(p.goals+p1.total_shots_Players) as goals, p.teamid\n",
    "\n",
    "from players p join player1 p1 on\n",
    "\n",
    "p.number =  p1.Player_Jersey_Num and p.teamid = p1.Team_ID\n",
    "'''\n",
    "updatePlayers=spark.sql(query)\n",
    "updatePlayers.show()\n",
    "#spark.sql(query).show(n =30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a048e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q14\n",
    "#Write pyspark code (in SQL or DataFrame API) to write the updated in question 11 to a new mssql.sidearmdb.players2 table.\n",
    "\n",
    "updatePlayers.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"players2\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q15\n",
    "#Re-write drill SQL query from question 1 to use the updated players2 and teams2 tables.\n",
    "\n",
    "select t.name as Team_Name,t.Wins as Team_Wins,t.losses as Team_Losses,\n",
    "p.name as Player_Name,p.shots as Player_Shots,p.goals as Player_Goals from teams2 t \n",
    "join players2 p on t.id = p.teamid\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
